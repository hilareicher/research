{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from cassis import *\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sympy.physics.control.control_plots import plt\n",
    "from bidi.algorithm import get_display\n",
    "import arabic_reshaper"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "iou_threshold = 1  # exact match\n",
    "\n",
    "def extract_labels_partial_match(annotations_a, annotations_b, all_labels_a, all_labels_b):\n",
    "    labels_a = []\n",
    "    labels_b = []\n",
    "    # Check each annotation in A against all in B for overlap\n",
    "    for a in annotations_a:\n",
    "        matched = False\n",
    "        for b in annotations_b:\n",
    "            if calculate_iou((a[0], a[1]), (b[0], b[1])) >= iou_threshold:\n",
    "                labels_a.append(a[2])  # Assuming the format is (start, end, label)\n",
    "                labels_b.append(b[2])\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            labels_a.append(a[2])\n",
    "            labels_b.append(\"O\")\n",
    "\n",
    "    # Repeat for annotations_b to catch any that weren't matched to annotations_a\n",
    "    for b in annotations_b:\n",
    "        if not any(calculate_iou((b[0], b[1]), (a[0], a[1])) >= iou_threshold for a in annotations_a):\n",
    "            labels_b.append(b[2])\n",
    "            labels_a.append(\"O\")\n",
    "    # add labels to the global list\n",
    "    all_labels_a.extend(labels_a)\n",
    "    all_labels_b.extend(labels_b)\n",
    "    return labels_a, labels_b\n",
    "\n",
    "\n",
    "# Intersection Over Union (IoU) for spans\n",
    "def calculate_iou(span_a, span_b):\n",
    "    \"\"\"Calculate the IoU of two spans.\"\"\"\n",
    "    intersection = max(0, min(span_a[1], span_b[1]) - max(span_a[0], span_b[0]))\n",
    "    union = max(span_a[1], span_b[1]) - min(span_a[0], span_b[0])\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "\n",
    "# output annotation summary for all docs of the form: annotation category, # annotations, % out of total\n",
    "# output the data to a file with annotator name\n",
    "def output_annotation_summary(ann_name, annotations):\n",
    "    total_annotations = len(annotations)\n",
    "    csvwriter = csv.writer(open(f\"{ann_name}_annotation_summary.csv\", \"w\"))\n",
    "    csvwriter.writerow([\"label\", \"# annotations\", \"% out of total\"])\n",
    "    annotation_counts = {}\n",
    "    for a in annotations:\n",
    "        if a[2] not in annotation_counts:\n",
    "            annotation_counts[a[2]] = 0\n",
    "        annotation_counts[a[2]] += 1\n",
    "    # sort by counts\n",
    "    annotation_counts = dict(sorted(annotation_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "    for k in annotation_counts:\n",
    "        # write to file\n",
    "        csvwriter.writerow([k, annotation_counts[k], annotation_counts[k] / total_annotations * 100])\n",
    "    csvwriter.writerow([\"Total\", total_annotations, \"100%\"])\n",
    "\n",
    "\n",
    "def check_label_agreement(label_a, label_b):\n",
    "    return label_a == label_b\n",
    "\n",
    "\n",
    "def count_tokens(text):\n",
    "    tokens = text.split()\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "def extract_annotations(cas):\n",
    "    annotations = []\n",
    "    for fs in cas.select_all():\n",
    "        if hasattr(fs, \"label\") and hasattr(fs, \"begin\") and hasattr(fs, \"end\") and fs.begin >= 0 and fs.end >= 0:\n",
    "            tokens_cnt = count_tokens(fs.get_covered_text())\n",
    "            # remove trailing - from label\n",
    "            if fs.label != None and fs.label.endswith(\"-\"):\n",
    "                fs.label = fs.label[:-1]\n",
    "            annotations.append((fs.begin, fs.end, fs.label, tokens_cnt))\n",
    "            print (f\"begin: {fs.begin}, end: {fs.end}, label: {fs.label}, tokens_cnt: {tokens_cnt}\")\n",
    "    # remove duplicates\n",
    "    annotations = list(set(annotations))\n",
    "    return annotations\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3375d0ba008e2c4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# inception_dir = os.environ['INCEPTION_FILES_DIR']\n",
    "inception_dir = \"./data-backup/files-1706796877\"\n",
    "if not os.path.isdir(inception_dir):\n",
    "    print(\"Error: INCEPTION_FILES_DIR is not a directory\")\n",
    "    exit(1)\n",
    "print(\"loading INCEpTION directory, path: \" + inception_dir)\n",
    "\n",
    "# load typesystem file\n",
    "xml_file = os.path.join(\".\", \"TypeSystem.xml\")\n",
    "if not os.path.isfile(xml_file):\n",
    "    print(\"Error: TypeSystem.xml file not found in INCEpTION directory\")\n",
    "    exit(1)\n",
    "print(\"loading typesystem file: \" + xml_file)\n",
    "with open(xml_file, 'rb') as f:\n",
    "    typesystem = load_typesystem(f)\n",
    "\n",
    "files_cnt = 0\n",
    "empty_annotation_cnt = 0\n",
    "total_annotation_instances = 0\n",
    "multiple_annotators_cnt = 0\n",
    "single_annotation_cnt = {}\n",
    "two_annotation_cnt = 0\n",
    "invalid_cnt = 0\n",
    "agreement_cnt = 0\n",
    "partial_agreement_cnt = 0\n",
    "annotations_tokens_sum = 0\n",
    "annotations_tokens_cnt = 0\n",
    "\n",
    "docs = {}\n",
    "annotations_of_annotator = {}\n",
    "annotations_of_annotator[\"CURATION_USER\"] = []\n",
    "curated_docs = {}\n",
    "# go over files and organize by document and annotator\n",
    "for f in os.listdir(inception_dir):\n",
    "    if f.endswith(\".xmi\"):\n",
    "        xmi_file = os.path.join(inception_dir, f)\n",
    "        # print(\"loading XMI file: \" + xmi_file)\n",
    "        with open(xmi_file, 'rb') as f:\n",
    "            cas = load_cas_from_xmi(f, typesystem=typesystem)\n",
    "        files_cnt += 1\n",
    "        # extract document title and annotator username from document metadata\n",
    "        md = cas.select(\"de.tudarmstadt.ukp.dkpro.core.api.metadata.type.DocumentMetaData\")\n",
    "        doc_id = md[0].documentTitle\n",
    "        annotator = md[0].documentId\n",
    "\n",
    "        annotations = extract_annotations(cas)\n",
    "        if len(annotations) == 0:\n",
    "            empty_annotation_cnt += 1\n",
    "            if DEBUG:\n",
    "                print(f\"Error: no annotations for document {doc_id} and annotator {annotator}, skipping ...\")\n",
    "            continue  # do not include in stats\n",
    "\n",
    "        if annotator == \"naama\":\n",
    "            invalid_cnt += 1\n",
    "            continue\n",
    "\n",
    "        if annotator == \"CURATION_USER\":\n",
    "            curated_docs[doc_id] = cas\n",
    "            continue\n",
    "\n",
    "        if doc_id not in docs:\n",
    "            docs[doc_id] = {}\n",
    "        if annotator not in docs[doc_id]:\n",
    "            docs[doc_id][annotator] = []\n",
    "        docs[doc_id][annotator].append(cas)\n",
    "\n",
    "        total_annotation_instances += 1  # without curated docs, Naama and empty annotations\n",
    "\n",
    "        for a in annotations:\n",
    "            annotations_tokens_sum += a[3]\n",
    "            annotations_tokens_cnt += 1\n",
    "\n",
    "all_labels_ab_a = []\n",
    "all_labels_ab_b = []\n",
    "all_labels_ac_a = []\n",
    "all_labels_bc_b = []\n",
    "all_labels_ac_c = []\n",
    "all_labels_bc_c = []\n",
    "\n",
    "total_number_of_tokens = 0\n",
    "total_number_of_annotated_tokens = 0\n",
    "total_number_of_annotations = 0\n",
    "individual_agreement_results = {}\n",
    "annotation_counts_totals = {}\n",
    "for doc_id in docs:\n",
    "    # get first CAS for document\n",
    "    first_cas = list(docs[doc_id].values())[0][0]\n",
    "    total_number_of_tokens += count_tokens(first_cas.get_sofa().sofaString)\n",
    "    print (\"doc_id \" + doc_id)\n",
    "    print (\"accumulated number of tokens \" + str(total_number_of_tokens))\n",
    "    annotations = extract_annotations(first_cas)\n",
    "    for a in annotations:\n",
    "        total_number_of_annotated_tokens += a[3]\n",
    "        total_number_of_annotations += 1\n",
    "    for a in annotations:\n",
    "        if a[2] not in annotation_counts_totals:\n",
    "            annotation_counts_totals[a[2]] = 0\n",
    "        annotation_counts_totals[a[2]] += 1\n",
    "    \n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"calculating agreement for document: \" + doc_id)\n",
    "    # if more than two annotators, skip document\n",
    "    if len(docs[doc_id]) > 2:  # should never happen\n",
    "        print(f\"More than two annotators for document {doc_id}, annotators are {docs[doc_id].keys()} skipping ...\")\n",
    "        multiple_annotators_cnt += 1\n",
    "        continue\n",
    "    # if only one annotator, skip document\n",
    "    if len(docs[doc_id]) < 2:\n",
    "        if DEBUG:\n",
    "            print(f\"One annotator for document {doc_id}, skipping ...\")\n",
    "        annotator = list(docs[doc_id].keys())[0]\n",
    "        if annotator not in single_annotation_cnt:\n",
    "            single_annotation_cnt[annotator] = 0\n",
    "        single_annotation_cnt[annotator] += 1\n",
    "        print (f\"{doc_id},{annotator},{len(annotations)}\")\n",
    "        continue\n",
    "    two_annotation_cnt += 1\n",
    "    # only if two annotators, calculate agreement\n",
    "    annotators = list(docs[doc_id].keys())\n",
    "    # sort annotators by name\n",
    "    annotators.sort()\n",
    "    \n",
    "    if (len(docs[doc_id][annotators[0]]) != 1) or (len(docs[doc_id][annotators[1]]) != 1):\n",
    "        print(f\"Error: more than one CAS for document {doc_id}, skipping ...\")\n",
    "        continue  # should never happen\n",
    "\n",
    "    cas_a = docs[doc_id][annotators[0]][0]\n",
    "    cas_b = docs[doc_id][annotators[1]][0]\n",
    "    annotations_a = extract_annotations(cas_a)\n",
    "    annotations_b = extract_annotations(cas_b)\n",
    "    if DEBUG:\n",
    "        print(\"the two annotators are \" + annotators[0] + \" and \" + annotators[1])\n",
    "    # if we have two annotators, store annotations for each annotator for stats\n",
    "    if annotators[0] not in annotations_of_annotator:\n",
    "        annotations_of_annotator[annotators[0]] = []\n",
    "    annotations_of_annotator[annotators[0]].append(annotations_a)\n",
    "\n",
    "    if annotators[1] not in annotations_of_annotator:\n",
    "        annotations_of_annotator[annotators[1]] = []\n",
    "    annotations_of_annotator[annotators[1]].append(annotations_b)\n",
    "\n",
    "    labels_a, labels_b = extract_labels_partial_match(annotations_a, annotations_b, all_labels_ab_a, all_labels_ab_b)\n",
    "    # calc agreement just for a single doc\n",
    "    if len(set(labels_a).union(labels_b)) == 1:\n",
    "        kappa_agreement_score = 1\n",
    "    else:\n",
    "        kappa_agreement_score = cohen_kappa_score(labels_a, labels_b)\n",
    "    f1_agreement_score = f1_score(labels_a, labels_b, average='weighted')\n",
    "    individual_agreement_results[doc_id] = (\n",
    "    kappa_agreement_score, f1_agreement_score, len(annotations_a), len(annotations_b), annotations_a, annotations_b,\n",
    "    labels_a, labels_b)\n",
    "\n",
    "    # also calculate agreement of each annotator with the curated doc\n",
    "    if doc_id in curated_docs.keys():\n",
    "        curated_doc = curated_docs[doc_id]\n",
    "        annotations_curated = extract_annotations(curated_doc)\n",
    "        annotations_of_annotator[\"CURATION_USER\"].append(annotations_curated) # for stats\n",
    "        extract_labels_partial_match(annotations_a, annotations_curated, all_labels_ac_a, all_labels_ac_c)\n",
    "        extract_labels_partial_match(annotations_b, annotations_curated, all_labels_bc_b, all_labels_bc_c)\n",
    "\n",
    "# calc agreement for all docs\n",
    "kappa_agreement_score_ab = cohen_kappa_score(all_labels_ab_a, all_labels_ab_b)\n",
    "f1_agreement_score_ab = f1_score(all_labels_ab_a, all_labels_ab_b, average='weighted')\n",
    "\n",
    "# calc kappa for each annotator with the curated doc)\n",
    "kappa_agreement_score_ac = cohen_kappa_score(all_labels_ac_a, all_labels_ac_c)\n",
    "kappa_agreement_score_bc = cohen_kappa_score(all_labels_bc_b, all_labels_bc_c)\n",
    "\n",
    "# calc f1 for each annotator with the curated doc\n",
    "f1_agreement_score_ac = f1_score(all_labels_ac_a, all_labels_ac_c, average='weighted')\n",
    "f1_agreement_score_bc = f1_score(all_labels_bc_b, all_labels_bc_c, average='weighted')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a9d0a7311aa2a11"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "labels = list(set(all_labels_ab_a).union(all_labels_ab_b))\n",
    "display_labels = [get_display(arabic_reshaper.reshape(label)) for label in labels]\n",
    "confusion_matrix_ab = confusion_matrix(all_labels_ab_a, all_labels_ab_b, labels=labels)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_ab, display_labels=display_labels)\n",
    "plt.figure(figsize=(12, 10), dpi=100)  # Adjust figsize and dpi as needed\n",
    "display.plot(cmap=plt.cm.Blues)\n",
    "plt.gcf().set_size_inches(12, 10)  # Ensure the figure size is applied correctly\n",
    "plt.xticks(rotation=45, ha=\"right\")  # Rotate labels and adjust alignment\n",
    "plt.tight_layout()  # Adjust layout to make room for label\n",
    "plt.ylabel(\"אסתר\")\n",
    "plt.xlabel(\"נטע \")\n",
    "plt.show()\n",
    "\n",
    "# calculate annotation counts for each annotator. Each summary is of the form: annotation category, # annotations, % out of total\n",
    "for annotator in annotations_of_annotator:\n",
    "    annotations = annotations_of_annotator[annotator]\n",
    "    print(f\"number of unique docs that were annotated by {annotator} annotator: {str(len(annotations))}\")\n",
    "    all_annotations = [item for sublist in annotations for item in sublist]\n",
    "    output_annotation_summary(annotator, all_annotations)\n",
    "    # print average number of annotations per document for each annotator\n",
    "    avg_annotations = len(all_annotations) / len(annotations)\n",
    "    print(f\"Average number of annotations per document for annotator {annotator}: {avg_annotations}\")\n",
    "\n",
    "print(\"Invalid annotator docs count: \" + str(invalid_cnt))\n",
    "print(\"Multiple annotators unique docs count: \" + str(multiple_annotators_cnt))\n",
    "print(\"Single annotator unique docs count: \" + str(single_annotation_cnt))\n",
    "print(\"Two annotators unique docs count: \" + str(two_annotation_cnt))\n",
    "\n",
    "# count curated_docs docs\n",
    "print(\"Curated docs count: \" + str(len(curated_docs)))\n",
    "\n",
    "# print average number of tokens per annotation\n",
    "avg_tokens = annotations_tokens_sum / annotations_tokens_cnt\n",
    "print(\"Average number of tokens per annotation: \" + str(avg_tokens))\n",
    "# number of documents that were annotated, if a document was annotated by more than one annotator, it is counted once for each annotator\n",
    "# does not include curated docs, includes docs that have at least one annotation\n",
    "print(\"Total annotation instances count: \" + str(total_annotation_instances))\n",
    "# files with no annotations\n",
    "print(\"Empty annotation instances count: \" + str(empty_annotation_cnt))\n",
    "print(\"Total files count: \" + str(files_cnt))\n",
    "print(f\"kappa: {str(kappa_agreement_score_ab)} ,iou_threshold: {iou_threshold}\")\n",
    "print(f\"f1 agreement: {str(f1_agreement_score_ab)} ,iou_threshold: {iou_threshold}\")\n",
    "# print curation agreement\n",
    "print(f\"kappa for curation user with annotator a: {str(kappa_agreement_score_ac)} ,iou_threshold: {iou_threshold}\")\n",
    "print(f\"kappa for curation user with annotator b: {str(kappa_agreement_score_bc)} ,iou_threshold: {iou_threshold}\")\n",
    "print(f\"f1 agreement for curation user with annotator a: {str(f1_agreement_score_ac)} ,iou_threshold: {iou_threshold}\")\n",
    "print(f\"f1 agreement for curation user with annotator b: {str(f1_agreement_score_bc)} ,iou_threshold: {iou_threshold}\")\n",
    "\n",
    "\n",
    "# save individual agreement results to csv file in the format: doc_id, kappa, f1, annotator1_annotations_count, annotator2_annotations_count\n",
    "# sort by kappa\n",
    "individual_agreement_results = dict(\n",
    "    sorted(individual_agreement_results.items(), key=lambda item: item[1][0], reverse=True))\n",
    "csvwriter = csv.writer(open(\"individual_agreement_results.csv\", \"w\"))\n",
    "csvwriter.writerow(\n",
    "    [\"doc_id\", \"kappa\", \"f1\", \"annotator1_annotations_count\", \"annotator2_annotations_count\", \"annotator1_annotations\",\n",
    "     \"annotator2_annotations\", \"labels_a\", \"labels_b\"])\n",
    "for doc_id in individual_agreement_results:\n",
    "    csvwriter.writerow([doc_id, individual_agreement_results[doc_id][0], individual_agreement_results[doc_id][1],\n",
    "                        individual_agreement_results[doc_id][2], individual_agreement_results[doc_id][3],\n",
    "                        individual_agreement_results[doc_id][4],\n",
    "                        individual_agreement_results[doc_id][5], individual_agreement_results[doc_id][6],\n",
    "                        individual_agreement_results[doc_id][7]])\n",
    "print(\"done\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77877b6f7fd2a3cb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
